## Notes

- Had meeting with Yomi, Chris, and Yongho about merging our code into the ECR application profiling repo.

- Could not get Colin's plugin to work with the memory requirements of the NX, so I am moving on to Seongha's cloud cover plugin. I setup the playback server to host the example image of some clouds at: `http://169.254.78.25:8090/top/image.jpg`

  - Am building and running the cloud cover plugin with this command:

    ```
    sudo docker build -t plugin-cloudcover-unet . && sudo docker run -it --runtime nvidia --network host -v /home/nvidia/luke_projects/config_dir/data-config.json:/run/waggle/data-config.json --name plugin-cloudcover -v /home/nvidia/live_metrics.sock:/metrics/live_metrics.sock plugin-cloudcover-unet -stream top_img -debug
    ```

- Meeting with Aji about papers

  - What is your preference of conference to submit to?
  - How is your feedback controller progress?
    - Any blockers that I can help with?
    - Memory issue caused by redundant DNN libraries being loaded by Docker? (i.e. TensorRT on the NX)
  - I am currently trying to get my live metrics code integrated into a few sample applications (Object counter, Cloud cover, another to be chosen...) How did you get the Anytime DNN setup? How are you receiving latency/fps data from it? If it is some native solution I want to try to see if my metrics reporting probes work on its inferencing function.
    - The ideal prototype setup for a paper demo would be running an Anytime DNN, another computer vision model that has rigorous science rules, and another app that can change its run configuration
  - You wanted the average latency/fps feature implemented into the metrics server, correct?
    - Can't latency and fps both be computed with the same measurement of time elapsed per inference?

