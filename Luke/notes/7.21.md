## Notes

- Testing out how I can package up my previous research on water segmentation algorithms. It would be a hassle to re-train the models I have (especially since I trained them on a Chameleon cluster), so I am going to use three models I have already trained. I trained three scikit RandomForestClassifiers to be what I call "TextureTemporal" water segmentation models. These models were trained at different framerates. The feature vector given to all of them is the same 60x800x600 tensor, but the spacing of the 60 frames is different. So far I have found that the 5fps model works well (although it looks like it struggles a little more to differentiate grasses from water than the 50fps model), so I am going to have that be the "standard" model that users can load to inference on video.
  - I have only worked with pre-recorded video, not a live stream, so I will need to figure out how to pass the right framerate to the model. I may want to look at implementing a queue that has a backlog of frames that I could pass all at once to the model. However, I will first need to see the latency of pulling frames through pywaggle, and how to GPU-accelerate the plugin too.
- I have setup a prototype app that exports tegrastats and the global GPU memory available on the NX. I will need to make a new branch and commit my code so that the others can see the general architecture of Prometheus.
  - Created a tutorial that lives at: https://github.com/waggle-sensor/application-profiling/tree/live-profiling/live_profiling

